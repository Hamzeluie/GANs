{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn as nn\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageDataset class for \"horse2zebra\" datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
    "        if len(self.files_A) > len(self.files_B):\n",
    "            self.files_A, self.files_B = self.files_B, self.files_A\n",
    "        self.new_perm()\n",
    "        assert len(self.files_A) > 0, \"Make sure you downloaded the horse2zebra images!\"\n",
    "\n",
    "    def new_perm(self):\n",
    "        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))\n",
    "        if item_A.shape[0] != 3:\n",
    "            item_A = item_A.repeat(3, 1, 1)\n",
    "        if item_B.shape[0] != 3:\n",
    "            item_B = item_B.repeat(3, 1, 1)\n",
    "        if index == len(self) - 1:\n",
    "            self.new_perm()\n",
    "        return (item_A - 0.5) * 2, (item_B - 0.5) * 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator has \n",
    "- two Feature map\n",
    "- two Contracting block \n",
    "- nine Residual block \n",
    "- two Expanding block\n",
    "\n",
    "Feature map contain:\n",
    "one convolution layer with kernel 7*7 and padding 3\n",
    "\n",
    "Contracting block contain:\n",
    "one convolution layer with kernel 3*3 , stride2 and padding 1\n",
    "\n",
    "Residual block contain:\n",
    "with respect of paper for 256*256 resolution use 9 block each block has two convolution with kernel 3*3 and  1 and\n",
    "instance normalizaion\n",
    "\n",
    "Expanding block contain:\n",
    "one convoluiotn transpose layer in order to upsample with half output channel than input channel and stride 2 and padding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMapBlock(nn.Module):\n",
    "    def __init__(self, input_channel, output_channels):\n",
    "        super(FeatureMapBlock, self).__init__()\n",
    "#         self.conv = nn.Conv2d(input_channel, output_channels, kernel_size=7, padding=3, adding_mode='reflect')\n",
    "        self.conv = nn.Conv2d(input_channel, output_channels, kernel_size=7, padding=3)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "    \n",
    "class ContractingBlock(nn.Module):\n",
    "    def __init__(self, input_channels, use_bn=True, kernel_size=3, activation='relu'):\n",
    "        super(ContractingBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=kernel_size,\n",
    "                               padding=1, stride=2, padding_mode='reflect')\n",
    "        self.activation = nn.ReLU() if activation == 'relu' else nn.LeakyReLU(0.2)\n",
    "        if use_bn:\n",
    "            self.instancenorm = nn.InstanceNorm2d(input_channels * 2)\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel, input_channel, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "        self.conv2 = nn.Conv2d(input_channel, input_channel, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "        self.instancenorm = nn.InstanceNorm2d(input_channel)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_x = x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.instancenorm(x)\n",
    "        return original_x + x\n",
    "\n",
    "    \n",
    "class ExpandingBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, use_bn=True):\n",
    "        super(ExpandingBlock, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(input_channels, input_channels // 2, kernel_size=3,\n",
    "                                        stride=2, padding=1, output_padding=1)\n",
    "        if use_bn:\n",
    "            self.instancenorm = nn.InstanceNorm2d(input_channels // 2)\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, hidden_channels=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2)\n",
    "        res_mult = 4\n",
    "        self.res0 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res1 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res2 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res3 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res4 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res5 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res6 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res7 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res8 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.expand2 = ExpandingBlock(hidden_channels * 4)\n",
    "        self.expand3 = ExpandingBlock(hidden_channels * 2)\n",
    "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x3 = self.res0(x2)\n",
    "        x4 = self.res1(x3)\n",
    "        x5 = self.res2(x4)\n",
    "        x6 = self.res3(x5)\n",
    "        x7 = self.res4(x6)\n",
    "        x8 = self.res5(x7)\n",
    "        x9 = self.res6(x8)\n",
    "        x10 = self.res7(x9)\n",
    "        x11 = self.res8(x10)\n",
    "        x12 = self.expand2(x11)\n",
    "        x13 = self.expand3(x12)\n",
    "        xn = self.downfeature(x13)\n",
    "        return self.tanh(xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# its time to define Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels, use_bn=False, kernel_size=4, activation='lrelu')\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2, kernel_size=4, activation='lrelu')\n",
    "        self.contract3 = ContractingBlock(hidden_channels * 4, kernel_size=4, activation='lrelu')\n",
    "        self.final = nn.Conv2d(hidden_channels * 8, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x3 = self.contract3(x2)\n",
    "        xn = self.final(x3)\n",
    "        return xn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for visualizing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_shifted = image_tensor\n",
    "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator loss contain\n",
    "\n",
    "adversarial loss + cycle consistency loss\n",
    "\n",
    "in paper introduce an additional \"Identity loss\" to encourage the mapping to preserve color composition between the input and output\n",
    "\n",
    "so loss would be:\n",
    "\n",
    "adversarial loss + cycle consistency loss + Identity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cycle_consistency_loss(x, y, gen, cycle_criterion):\n",
    "    gen = gen(y)\n",
    "    loss = cycle_criterion(gen, x)\n",
    "    return loss, gen\n",
    "\n",
    "\n",
    "def get_adversarial_loss(x, disc, gen, adv_criterion):\n",
    "    fake = gen(x)\n",
    "    disc_hat = disc(fake)\n",
    "    loss = adv_criterion(disc_hat, torch.ones_like(disc_hat))\n",
    "    return loss, fake\n",
    "\n",
    "\n",
    "def get_identity_loss(x, gen, identity_criterion):\n",
    "    identity = gen(x)\n",
    "    loss = identity_criterion(identity, x)\n",
    "    return loss, identity\n",
    "\n",
    "def get_gen_loss(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, identity_criterion, cycle_criterion,\n",
    "             lambda_identity=0.1, lambda_cycle=10):\n",
    "    # adversarial Loss\n",
    "    adv_loss_BA, fake_A = get_adversarial_loss(real_B, disc_A, gen_BA, adv_criterion)\n",
    "    adv_loss_AB, fake_B = get_adversarial_loss(real_A, disc_B, gen_AB, adv_criterion)\n",
    "    gen_adversarial_loss = adv_loss_BA + adv_loss_AB\n",
    "    # Identity loss\n",
    "    identity_loss_A, identity_A = get_identity_loss(real_A, gen_BA, identity_criterion)\n",
    "    identity_loss_B, identity_B = get_identity_loss(real_B, gen_AB, identity_criterion)\n",
    "    gen_identity_loss = identity_loss_A + identity_loss_B\n",
    "    # cycle consistency loss\n",
    "    cycle_loss_BA, cycle_A = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)\n",
    "    cycle_loss_AB, cycle_B = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)\n",
    "    gen_cycle_loss = cycle_loss_BA + cycle_loss_AB\n",
    "    # Total Loss\n",
    "    loss = lambda_identity * gen_identity_loss + lambda_cycle * gen_cycle_loss + gen_adversarial_loss\n",
    "    return loss, fake_A, fake_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_loss(real_X, fake_X, disc_X, adv_criterion):\n",
    "    disc_fake_X_hat = disc_X(fake_X.detach())\n",
    "    disc_fake_X_loss = adv_criterion(disc_fake_X_hat, torch.zeros_like(disc_fake_X_hat))\n",
    "    disc_real_X_hat = disc_X(real_X)\n",
    "    disc_real_X_loss = adv_criterion(disc_real_X_hat, torch.ones_like(disc_real_X_hat))\n",
    "    disc_loss = (disc_fake_X_loss + disc_real_X_loss) / 2\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Befor going to next step lets test functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_X = torch.tensor(83.)\n",
    "test_fake_X = torch.tensor(89.)\n",
    "test_fake_Y = torch.tensor(97.)\n",
    "\n",
    "\n",
    "test_disc_Y = lambda x: x * 97\n",
    "test_disc_X = lambda x: x * 97\n",
    "\n",
    "test_gen_XY = lambda x: x * 89\n",
    "test_gen_YX = lambda x: x * 89\n",
    "\n",
    "\n",
    "test_cycle_criterion = lambda x, y: (x + y) * 73\n",
    "test_adv_criterion = lambda x, y: x * 79 + y * 73\n",
    "test_identity_criterion = lambda x, y: (x + y) * 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Discriminator Loss!\n",
      "Success Adversarial Loss!\n",
      "Success Identity Loss!\n",
      "Success Cycle Consistency Loss!\n"
     ]
    }
   ],
   "source": [
    "#Discriminator Loss\n",
    "assert torch.abs((get_disc_loss(test_real_X, test_fake_X, test_disc_X, test_adv_criterion)) - 659054.5000) < 1e-6\n",
    "print(\"Success Discriminator Loss!\")\n",
    "#Adversarial Loss\n",
    "test_res = get_adversarial_loss(test_real_X, test_disc_Y, test_gen_XY, test_adv_criterion)\n",
    "assert torch.abs(test_res[0] - 56606652) < 1e-6\n",
    "assert torch.abs(test_res[1] - 7387) < 1e-6\n",
    "print(\"Success Adversarial Loss!\")\n",
    "#Identity Loss\n",
    "test_res = get_identity_loss(test_real_X, test_gen_YX, test_identity_criterion)\n",
    "assert torch.abs(test_res[0] - 545310) < 1e-6\n",
    "assert torch.abs(test_res[1] - 7387) < 1e-6\n",
    "print(\"Success Identity Loss!\")\n",
    "#Cycle Consistency Loss\n",
    "test_res = get_cycle_consistency_loss(test_real_X, test_fake_Y, test_gen_YX, test_cycle_criterion)\n",
    "assert torch.abs(test_res[1] - 8633) < 1e-6\n",
    "assert torch.abs(test_res[0] - 636268) < 1e-6\n",
    "print(\"Success Cycle Consistency Loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Generator Loss!\n"
     ]
    }
   ],
   "source": [
    "test_real_A = torch.tensor(97)\n",
    "test_real_B = torch.tensor(89)\n",
    "test_gen_AB = lambda x: x * 83\n",
    "test_gen_BA = lambda x: x * 79\n",
    "test_disc_A = lambda x: x * 47\n",
    "test_disc_B = lambda x: x * 43\n",
    "test_adv_criterion = lambda x, y: x * 73 + y * 71\n",
    "test_recon_criterion = lambda x, y: (x + y) * 61\n",
    "test_lambda_identity = 59\n",
    "test_lambda_cycle = 53\n",
    "test_res = get_gen_loss(\n",
    "    test_real_A, \n",
    "    test_real_B, \n",
    "    test_gen_AB, \n",
    "    test_gen_BA, \n",
    "    test_disc_A,\n",
    "    test_disc_B,\n",
    "    test_adv_criterion, \n",
    "    test_recon_criterion, \n",
    "    test_recon_criterion, \n",
    "    test_lambda_identity, \n",
    "    test_lambda_cycle)\n",
    "assert test_res[0].item() == 4047804560\n",
    "assert test_res[1].item() == 7031\n",
    "assert test_res[2].item() == 8051\n",
    "print(\"Success Generator Loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"horse2zebra\"\n",
    "n_epochs = 20\n",
    "dim_A = 3\n",
    "dim_B = 3\n",
    "display_step = 200\n",
    "load_shape = 286\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "load_shape = 286\n",
    "target_shape = 256\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_criterion = nn.MSELoss()\n",
    "recon_criterion = nn.L1Loss()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(load_shape),\n",
    "    transforms.RandomCrop(target_shape),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = ImageDataset(path, transform=transform)\n",
    "\n",
    "\n",
    "gen_AB = Generator(dim_A, dim_B).to(device)\n",
    "gen_BA = Generator(dim_B, dim_A).to(device)\n",
    "gen_opt = torch.optim.Adam(list(gen_AB.parameters()) + list(gen_BA.parameters()), lr=lr, betas=(0.5, 0.999))\n",
    "disc_A = Discriminator(dim_A).to(device)\n",
    "disc_A_opt = torch.optim.Adam(disc_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "disc_B = Discriminator(dim_B).to(device)\n",
    "disc_B_opt = torch.optim.Adam(disc_B.parameters(), lr=lr, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial weight with normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(save_model=False):\n",
    "    mean_generator_loss = 0\n",
    "    mean_discriminator_loss = 0\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    cur_step = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Dataloader returns the batches\n",
    "        # for image, _ in tqdm(dataloader):\n",
    "        for real_A, real_B in tqdm(dataloader):\n",
    "            # image_width = image.shape[3]\n",
    "            real_A = nn.functional.interpolate(real_A, size=target_shape)\n",
    "            real_B = nn.functional.interpolate(real_B, size=target_shape)\n",
    "            cur_batch_size = len(real_A)\n",
    "            real_A = real_A.to(device)\n",
    "            real_B = real_B.to(device)\n",
    "\n",
    "            ### Update discriminator A ###\n",
    "            disc_A_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad():\n",
    "                fake_A = gen_BA(real_B)\n",
    "            disc_A_loss = get_disc_loss(real_A, fake_A, disc_A, adv_criterion)\n",
    "            disc_A_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_A_opt.step() # Update optimizer\n",
    "\n",
    "            ### Update discriminator B ###\n",
    "            disc_B_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad():\n",
    "                fake_B = gen_AB(real_A)\n",
    "            disc_B_loss = get_disc_loss(real_B, fake_B, disc_B, adv_criterion)\n",
    "            disc_B_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_B_opt.step() # Update optimizer\n",
    "\n",
    "            ### Update generator ###\n",
    "            gen_opt.zero_grad()\n",
    "            gen_loss, fake_A, fake_B = get_gen_loss(\n",
    "                real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, recon_criterion, recon_criterion\n",
    "            )\n",
    "            gen_loss.backward() # Update gradients\n",
    "            gen_opt.step() # Update optimizer\n",
    "\n",
    "            # Keep track of the average discriminator loss\n",
    "            mean_discriminator_loss += disc_A_loss.item() / display_step\n",
    "            # Keep track of the average generator loss\n",
    "            mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "            ### Visualization code ###\n",
    "            if cur_step % display_step == 0:\n",
    "                print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
    "                show_tensor_images(torch.cat([real_A, real_B]), size=(dim_A, target_shape, target_shape))\n",
    "                show_tensor_images(torch.cat([fake_B, fake_A]), size=(dim_B, target_shape, target_shape))\n",
    "                mean_generator_loss = 0\n",
    "                mean_discriminator_loss = 0\n",
    "                # You can change save_model to True if you'd like to save the model\n",
    "                if save_model:\n",
    "                    torch.save({\n",
    "                        'gen_AB': gen_AB.state_dict(),\n",
    "                        'gen_BA': gen_BA.state_dict(),\n",
    "                        'gen_opt': gen_opt.state_dict(),\n",
    "                        'disc_A': disc_A.state_dict(),\n",
    "                        'disc_A_opt': disc_A_opt.state_dict(),\n",
    "                        'disc_B': disc_B.state_dict(),\n",
    "                        'disc_B_opt': disc_B_opt.state_dict()\n",
    "                    }, f\"cycleGAN_{cur_step}.pth\")\n",
    "            cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
